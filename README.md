# Лабораторная работа № 2
## Список выполнивших

1. Боковой Владислав Сергеевич
2. Новая Дарья Александровна
3. Ершов Алексей Андреевич
## 1.Цель работы

Научиться работать с предобученными моделями и на основе предобученных эмбеддингов строить новые модели.

---
## 2.Анализ предметной области
# Трансформеры

Трансформеры — это архитектура нейронных сетей, предложенная в статье "Attention is All You Need" (2017), авторами которой являются Ашиш Васвани и его коллеги из Google Research. Трансформеры стали революцией в области обработки естественного языка (NLP) и широко используются в современных моделях, таких как BERT, GPT, T5 и других.

## Основная идея трансформеров

Трансформеры отказались от использования рекуррентных нейронных сетей (RNN) и сверточных сетей (CNN), которые раньше были основными подходами для обработки последовательностей, таких как текст. Вместо этого архитектура трансформеров основана на механизме внимания (Attention), который позволяет модели эффективно учитывать все элементы входной последовательности одновременно.

## Ключевые особенности трансформеров

### Механизм внимания (Attention)

В отличие от традиционных RNN и CNN, где информация обрабатывается поэтапно (например, одна по одной), трансформеры используют механизм внимания, который позволяет каждому элементу последовательности напрямую взаимодействовать с другими элементами. Таким образом, модель может учитывать долгосрочные зависимости между элементами.

Механизм **self-attention** позволяет вычислять важность каждого слова относительно других слов в предложении. Например, в предложении *"Она купила машину"*, модель может понять, что "она" относится к "женщине", и "машина" — это объект, с которым связано действие "купила".

### Параллелизация

Благодаря использованию внимания, все токены могут быть обработаны параллельно, что значительно ускоряет обучение. В RNN, например, информация передается от одного токена к следующему, что ограничивает параллелизацию.

### Отсутствие рекуррентных слоев (RNN)

Трансформеры не используют рекуррентные нейронные сети (RNN) или свёрточные сети (CNN), что позволяет устранить многие их ограничения, такие как проблемы с долгосрочной зависимостью в RNN. Рекуррентные сети требуют последовательной обработки, а трансформеры — нет.

### Позиционные эмбеддинги

Поскольку трансформеры не обрабатывают данные поэтапно (как RNN), они не могут напрямую учитывать порядок слов в предложении. Для этого добавляются **позиционные эмбеддинги**, которые добавляют информацию о позиции каждого токена в последовательности.

## Архитектура трансформера

Архитектура трансформера состоит из двух основных частей:

- **Энкодер (Encoder)**: Эта часть преобразует входную последовательность в скрытые представления. Энкодер состоит из нескольких одинаковых слоёв, каждый из которых включает механизм внимания и полносвязную сеть (Feed-Forward Network).
  
- **Декодер (Decoder)**: Эта часть генерирует выходную последовательность, используя скрытые представления, полученные от энкодера, и собственное внимание на основе предыдущих токенов. Декодер также состоит из нескольких одинаковых слоёв.

Каждый слой включает:

- Механизм **multi-head attention**, который разделяет внимание на несколько "голов" для выявления различных взаимосвязей между элементами.
- Полносвязная сеть (Feed-Forward Network), которая обрабатывает каждый токен по отдельности.

## Когда была предложена архитектура трансформеров?

Архитектура трансформеров была предложена в статье "Attention is All You Need" в 2017 году, что стало важной вехой в области обработки естественного языка (NLP). Этот подход оказался настолько эффективным, что вскоре он стал основой для многих моделей, таких как BERT (Bidirectional Encoder Representations from Transformers), GPT (Generative Pretrained Transformer), T5 (Text-to-Text Transfer Transformer) и других.

Модели, основанные на трансформерах, продемонстрировали значительные улучшения в задачах, таких как:

- Машинный перевод
- Классификация текста
- Генерация текста
- Ответы на вопросы
- Резюмирование текста

# Алгоритм работы трансформера

Алгоритм работы трансформера включает несколько этапов, которые происходят последовательно, но эффективно и параллельно. Ниже описаны ключевые шаги, используемые для обучения и предсказания.

---

## Входные данные

- **Текст:** Представлен последовательностью слов или символов, которые нужно токенизировать (разделить на токены).
- **Эмбеддинги:** Каждый токен преобразуется в числовое представление с помощью эмбеддингов (например, word embeddings или subword embeddings, таких как WordPiece или BPE).
- **Позиционные эмбеддинги:** Добавляются к токенам для учета порядка слов, поскольку трансформеры не обрабатывают данные последовательно, как RNN.

---

## Энкодер

Энкодер состоит из нескольких слоев (обычно 6–12), каждый из которых включает два основных компонента:

### 1. Механизм внимания (Attention Mechanism)

- **Цель:** Позволяет каждому токену взаимодействовать с другими токенами в последовательности.
- **Входные компоненты:** 
  - **Запросы (Q)**
  - **Ключи (K)**
  - **Значения (V)**
- **Self-Attention:** Механизм самовнимания, позволяющий учитывать важность каждого другого токена в предложении.
- **Формула внимания:**

  $$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

  где:
  - **Q** — запросы,
  - **K** — ключи,
  - **V** — значения,
  - **\(d_k\)** — размерность ключей.

- **Multi-head Attention:** Разделяет внимание на несколько "голов", позволяя извлекать различные аспекты зависимостей между токенами.

### 2. Feed-Forward Network (FFN)

После механизма внимания токены проходят через полносвязную сеть, которая обрабатывает каждый токен независимо и улучшает его представление.

### 3. Нормализация

На каждом слое применяется **Layer Normalization** для стабилизации обучения.

*Эти шаги помогают энкодеру создать скрытые представления входных токенов, содержащие всю необходимую информацию о контексте.*

---

## Декодер

Декодер состоит из нескольких слоев, аналогичных энкодеру, но с дополнительным механизмом **cross-attention** (перекрестного внимания).

### Основные компоненты декодера

- **Self-Attention:** Как и в энкодере, каждый токен в декодере взаимодействует с другими токенами в текущем контексте.
- **Cross-Attention:** Позволяет декодеру учитывать выходы энкодера, фокусируясь не только на предыдущих токенах, но и на скрытых представлениях, полученных от энкодера (важно для задач, таких как машинный перевод).
- **Feed-Forward Network:** Токены проходят через полносвязную сеть после внимания.
- **Нормализация:** Применяется **Layer Normalization** для улучшения стабильности модели.

*Декодер генерирует выходную последовательность, основываясь на скрытых представлениях энкодера и предыдущих токенах.*

---

## Выходные данные

После работы декодера выходные данные (предсказания) проходят через линейный слой и **softmax-активацию** для получения вероятностей принадлежности к каждому классу или слову. Это используется для задач, таких как:

- **Генерация текста**
- **Классификация текста**

---

## Предсказания

- **Для перевода или генерации текста:** Выход декодера используется для предсказания следующего токена в последовательности.
- **Для классификации:** Выход декодера преобразуется в итоговый класс (например, метку категории текста).
---
# Модели BERT и ALBERT

## **BERT** (Bidirectional Encoder Representations from Transformers)

**BERT** — модель, разработанная Google в 2018 году, которая анализирует текст в обоих направлениях одновременно. Благодаря такому подходу она лучше понимает контекст слов, обучаясь на следующих задачах:

- **Masked Language Modeling** (Задача замены случайных токенов)
- **Next Sentence Prediction** (Предсказание следующего предложения)

Этот подход позволяет модели учитывать контекст как слева, так и справа от каждого слова, что значительно улучшает её способность к пониманию.

---

## **ALBERT** (A Lite BERT)

**ALBERT** — облегчённая версия BERT, которая снижает количество параметров за счёт:

- Разделения эмбеддингов
- Совместного использования весов между слоями

Это делает модель более эффективной по памяти и быстрее обучаемой, при этом сохраняя высокую производительность в задачах обработки естественного языка.
# . Алгоритм работы разработанной системы

## Подготовка данных

1. **Приведение текста к нижнему регистру**: Изначальный текст преобразуется в нижний регистр.
2. **Удаление пунктуации и чисел**: Все знаки препинания и числа удаляются из текста.
3. **Удаление стоп-слов**: Используется список стоп-слов, загружаемый через библиотеку `nltk`, для удаления ненужных слов.
4. **Удаление эмодзи**: Эмодзи удаляются из текста.
5. **Стемминг слов**: Производится стемминг слов для приведения их к базовой форме.

---

## Подготовка данных

1. **Загрузка очищенного CSV файла**: Очищенный CSV файл считывается в `DataFrame`.
2. **Валидация колонок**: Проверка на наличие необходимых колонок в данных.
3. **Маппинг категорий**: Текстовые категории преобразуются в числовые метки, с помощью словаря соответствий.

---

## Конфигурация предобученной модели ALBERT

1. **Использование модели ALBERT**:
   - `AlbertTokenizer` для токенизации текстов.
   - `AlbertForSequenceClassification` для классификации.

2. **Параметры конфигурации**:
   - `model_name = "albert-base-v2"` — модель ALBERT базового размера.
   - `max_length = 128` — максимальная длина токенов.
   - `batch_size = 16` — размер мини-батча для предсказания.

---

## Токенизация

1. **Преобразования данных**:
   - Применяются операции **паддинга**, **усечения** и **преобразования в тензоры** с использованием PyTorch.
   
2. **Отправка данных на устройство**: Токенизированные данные отправляются на устройство (GPU или CPU) для дальнейшей обработки.

---

## Обучение

1. **Процесс классификации**:
   - Модель `AlbertForSequenceClassification` принимает токенизированные тексты и возвращает логиты — вероятности принадлежности к каждому классу.
   
2. **Обработка мини-батчей**:
   - Для каждой группы токенов из мини-батча вычисляется индекс класса с максимальной вероятностью.
   - Все предсказания аккумулируются в список.

---

## Вычисление метрик

1. **Precision**: Доля корректно предсказанных положительных примеров.
2. **Recall**: Доля корректно обнаруженных положительных примеров среди всех реальных.
3. **F1 Score**: Гармоническое среднее между **precision** и **recall**.
---
# 3. Результат выполнения программы

## Маппинг категорий

`{'travel': 0, 'food': 1, 'art_music': 2, 'history': 3}`

## Эпохи и потери

- **Эпоха 1**, Потеря: 67.8899
- **Эпоха 2**, Потеря: 23.7842
- **Эпоха 3**, Потеря: 20.0438

## Метрики

- **Точность (Precision):** 0.9626
- **Полнота (Recall):** 0.9625
- **F1-метрика:** 0.9625
- **Точность классификации (Accuracy):** 0.9625
---
# 4. Использованные источники

1. Васвани, А., Шазир, Н., Пармар, Н., Узкорет, Дж., Джонс, Л., Гомес, А. А., Кайзер, Л., Полосухин, И. (2017). *Attention is All You Need*. В сборнике: Прогресс в обработке нейронных сетей для систем обработки информации (NeurIPS), том 30. [Ссылка на статью](https://arxiv.org/abs/1706.03762)

2. Девлин, Дж., Чанг, М. В., Ли, К., Тоутадова, К. (2018). *BERT: Предобучение глубоких двунаправленных трансформеров для понимания языка*. [arXiv:1810.04805](https://arxiv.org/abs/1810.04805)

3. Лан, З., Чен, М., Гудман, С., Гимпель, К., Шарма, П., Сорикут, Р. (2019). *ALBERT: Легкий BERT для самоконтролируемого обучения языковых представлений*. [arXiv:1909.11942](https://arxiv.org/abs/1909.11942)

4. Жулин, А., Грейв, Э., Миколов, Т., и др. (2017). *Набор трюков для эффективной классификации текста*. [arXiv:1607.01759](https://arxiv.org/abs/1607.01759)

5. Голдберг, Й. (2017). *Методы нейронных сетей для обработки естественного языка*. Издательство Morgan & Claypool Publishers.
